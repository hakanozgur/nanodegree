{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\r\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cog = pd.read_csv(\"./data/0_raw/cy07_msu_stu_cog.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(606627, 3590)"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cog.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate cognitive question scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Going through codebook file I realized that some questions has partial credits.\r\n",
    "- Most of them were true false(1-0), some of them had only one partial credit, some had many.\r\n",
    "- Luckly there is a pattern to it\r\n",
    "\r\n",
    "\r\n",
    "- if score starts with 0 (0, 01, 02..) it means no credit\r\n",
    "- if score is not 1 but starts with 1 (11, 12, 13..) it means partial credit\r\n",
    "- if score is 1 or starts with 2 (1, 2, 21, 22..) it means full credit\r\n",
    "\r\n",
    "\r\n",
    "- Cognative items ids collected from compendia here : https://webfs.oecd.org/pisa2018/Compendia_Cognitive.zip\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Math questions\r\n",
    "math_question_ids = []\r\n",
    "with open(\"./data/reference/math_question_ids.txt\") as math_file:\r\n",
    "    for line in math_file:\r\n",
    "        math_question_ids.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading questions\r\n",
    "reading_question_ids = []\r\n",
    "with open(\"./data/reference/reading_question_ids.txt\") as reading_file:\r\n",
    "    for line in reading_file:\r\n",
    "        reading_question_ids.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Science questisons\r\n",
    "science_question_ids = []\r\n",
    "with open(\"./data/reference/science_question_ids.txt\") as science_file:\r\n",
    "    for line in science_file:\r\n",
    "        science_question_ids.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(606627, 795)"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# processing df row by row in jupyter notebook requires 64gb+ mem\r\n",
    "# drop columns not planning to use to free memory\r\n",
    "cog_cols = math_question_ids + reading_question_ids + science_question_ids\r\n",
    "useful_cols = ['CNT','CNTRYID', 'CNTSCHID', 'CNTSTUID', 'STRATUM', 'RCORE_PERF', 'RCO1S_PERF', 'LANGTEST_COG']\r\n",
    "df_cog.drop(columns=df_cog.columns.difference(cog_cols + useful_cols), inplace=True)\r\n",
    "df_cog.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- each question has a difficulty. however this difficulty is not provided within the dataset\r\n",
    "- among 400 reading question, only ~30 of them graded http://www.oecd.org/pisa/test/PISA2018_Released_REA_Items_12112019.pdf\r\n",
    "- this publication describes how leveling system changed in 2018 and what the levels in proficiency means. https://www.oecd-ilibrary.org/sites/5f07c754-en/1/2/6/index.html?itemId=/content/publication/5f07c754-en&_csp_=6aa84fb981b29e81b35b3f982f80670e&itemIGO=oecd&itemContentType=book#s49\r\n",
    "- according to the document only limited number of items are relased to public\r\n",
    "- https://www.oecd-ilibrary.org/sites/5f07c754-en/1/2/14/index.html?itemId=/content/publication/5f07c754-en&_csp_=6aa84fb981b29e81b35b3f982f80670e&itemIGO=oecd&itemContentType=book#mh199\r\n",
    "- I couldn't find the difficulties of the questions in data explorer, code book, compendia or in the dataset. I spend quite a lot of time I will get back to it again after some time\r\n",
    "- with difficulties I could have got more accurate scoring :(\r\n",
    "\r\n",
    "--\r\n",
    "\r\n",
    "- 2 days later examining data manually found a cluster of 300-500 values (which is the avg score)\r\n",
    "- apparently student questionnaire file has the scores to the cognative tests\r\n",
    "- after finding the variable i found out about their calculation and the reason behind this weighted approach here -> https://www.oecd.org/pisa/data/pisa2018technicalreport/PISA2018%20TecReport-Ch-19-Data-Products.pdf\r\n",
    "- in the code book going trough hundereds of columns I didn't expect scores to be in questionnare and at the end of the file \r\n",
    "\r\n",
    "--\r\n",
    "\r\n",
    "- 1 more day later\r\n",
    "- there was actully a link about \"How to prepare and analyse the PISA database\" ü§¶‚Äç‚ôÇÔ∏è\r\n",
    "- http://www.oecd.org/pisa/data/httpoecdorgpisadatabase-instructions.htm\r\n",
    "- There is a lengthy explanation to how scores are calculated\r\n",
    "  - Rasch Item Response Theory used before to calculate the weight of different problems (likelyhood, and bayesian models used)\r\n",
    "  - this response theory is good for questionnaires but cognitive tasks PISA uses plausible values PVs\r\n",
    "  - PVs used to split students into proficiency levels. and evaluate them in their bucket more accurately. looks at the population (country-economy), creates samplings, makes regression models and avgs them. I kind a get the main idea but it couldn't follow how it is calculated and the main difference between PV1-PV2.\r\n",
    "  - I will be using these PV values\r\n",
    "  - One of the documents mentions that correlations between domains (math, reading, science) should be calculated in the same plasuibel values (PV1Math vs PV1Reading) since PVs calculated from conditional posterior distributions, mixing them is not right\r\n",
    "  - In a contest PISA made they also mention for exploration it is fine to use only PV1 https://www.oecd.org/education/datavisualizationcontest.htm \r\n",
    "  - Crating std of PV1-PV10 doesn't seem right. but also there is value in other PV values, which I have yet to discover. I will probably use PV1 for now\r\n",
    "  - Maybe correlating between PVs in the same domain could give more insight. if their correlation value is similar across all PVs then using only one would be enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given column ids as question_ids this method calculates score of a row\r\n",
    "# valid answers given 10 point, partial answers given5 points\r\n",
    "def calculate_score_for_columns(row, question_ids):\r\n",
    "    student_score = 0.0\r\n",
    "    student_answered_questions = 0\r\n",
    "\r\n",
    "    for question in question_ids:\r\n",
    "        score = row[question]\r\n",
    "        if not np.isnan(score):\r\n",
    "            score_str = str(int(score))\r\n",
    "            student_answered_questions += 1\r\n",
    "            if score_str == \"1\" or score_str.startswith(\"2\"):\r\n",
    "                student_score += 10\r\n",
    "            elif score_str.startswith(\"1\"):\r\n",
    "                student_score += 5 # partial answer, half point\r\n",
    "    return student_score, student_answered_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_test_score(row):\r\n",
    "    m_score, m_count = calculate_score_for_columns(row, math_question_ids)\r\n",
    "    r_score, r_count = calculate_score_for_columns(row, reading_question_ids)\r\n",
    "    s_score, s_count = calculate_score_for_columns(row, science_question_ids)\r\n",
    "    return pd.Series([m_score, m_count, r_score, r_count, s_score, s_count])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate scores and answer counts\r\n",
    "score_columns = [\"math_score\", \"math_answered\", \"reading_score\", \"reading_answered\", \"science_score\", \"science_answered\"]\r\n",
    "df_cog[score_columns] = df_cog.apply(lambda row: calculate_test_score(row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_keep = useful_cols + score_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop unused columns\r\n",
    "df_cog.drop(columns=df_cog.columns.difference(columns_to_keep), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>CNTRYID</th>\n      <th>CNT</th>\n      <th>CNTSCHID</th>\n      <th>CNTSTUID</th>\n      <th>STRATUM</th>\n      <th>LANGTEST_COG</th>\n      <th>RCORE_PERF</th>\n      <th>RCO1S_PERF</th>\n      <th>math_score</th>\n      <th>math_answered</th>\n      <th>reading_score</th>\n      <th>reading_answered</th>\n      <th>science_score</th>\n      <th>science_answered</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>130012</th>\n      <td>170.0</td>\n      <td>COL</td>\n      <td>17000251.0</td>\n      <td>17003049.0</td>\n      <td>COL0410</td>\n      <td>156.0</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>350.0</td>\n      <td>55.0</td>\n      <td>160.0</td>\n      <td>36.0</td>\n    </tr>\n    <tr>\n      <th>59298</th>\n      <td>70.0</td>\n      <td>BIH</td>\n      <td>7000040.0</td>\n      <td>7004870.0</td>\n      <td>BIH0028</td>\n      <td>192.0</td>\n      <td>3.0</td>\n      <td>3.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>540.0</td>\n      <td>57.0</td>\n      <td>280.0</td>\n      <td>38.0</td>\n    </tr>\n    <tr>\n      <th>539152</th>\n      <td>784.0</td>\n      <td>ARE</td>\n      <td>78400114.0</td>\n      <td>78415262.0</td>\n      <td>ARE0659</td>\n      <td>313.0</td>\n      <td>3.0</td>\n      <td>3.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>380.0</td>\n      <td>56.0</td>\n      <td>260.0</td>\n      <td>39.0</td>\n    </tr>\n    <tr>\n      <th>94044</th>\n      <td>124.0</td>\n      <td>CAN</td>\n      <td>12400300.0</td>\n      <td>12406464.0</td>\n      <td>CAN0547</td>\n      <td>493.0</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>125.0</td>\n      <td>22.0</td>\n      <td>365.0</td>\n      <td>57.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>167411</th>\n      <td>214.0</td>\n      <td>DOM</td>\n      <td>21400196.0</td>\n      <td>21404617.0</td>\n      <td>DOM0003</td>\n      <td>156.0</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>280.0</td>\n      <td>58.0</td>\n      <td>90.0</td>\n      <td>39.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "        CNTRYID  CNT    CNTSCHID    CNTSTUID  STRATUM  LANGTEST_COG  \\\n130012    170.0  COL  17000251.0  17003049.0  COL0410         156.0   \n59298      70.0  BIH   7000040.0   7004870.0  BIH0028         192.0   \n539152    784.0  ARE  78400114.0  78415262.0  ARE0659         313.0   \n94044     124.0  CAN  12400300.0  12406464.0  CAN0547         493.0   \n167411    214.0  DOM  21400196.0  21404617.0  DOM0003         156.0   \n\n        RCORE_PERF  RCO1S_PERF  math_score  math_answered  reading_score  \\\n130012         2.0         2.0         0.0            0.0          350.0   \n59298          3.0         3.0         0.0            0.0          540.0   \n539152         3.0         3.0         0.0            0.0          380.0   \n94044          2.0         2.0       125.0           22.0          365.0   \n167411         2.0         1.0         0.0            0.0          280.0   \n\n        reading_answered  science_score  science_answered  \n130012              55.0          160.0              36.0  \n59298               57.0          280.0              38.0  \n539152              56.0          260.0              39.0  \n94044               57.0            0.0               0.0  \n167411              58.0           90.0              39.0  "
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cog.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cog.to_csv(\"./data/1_processed/cognitive_scores.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit (conda)",
   "metadata": {
    "interpreter": {
     "hash": "d3ff5a1ce451dfff603f0aeb2fe958ed2741cc1700d7ef242fbe9997037b9eb5"
    }
   },
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}